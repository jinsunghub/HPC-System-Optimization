{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsunghub/HPC-System-Optimization/blob/main/GEMM_Convolution_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기본 정보"
      ],
      "metadata": {
        "id": "b38fHvRZongo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile basic_info.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "int main() {\n",
        "    int deviceId;\n",
        "    cudaError_t err = cudaGetDevice(&deviceId);\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"Failed to get CUDA device: %s\\n\", cudaGetErrorString(err));\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    cudaDeviceProp props;\n",
        "    err = cudaGetDeviceProperties(&props, deviceId);\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"Failed to get device properties: %s\\n\", cudaGetErrorString(err));\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    printf(\"--- GPU Device: %s ---\\n\", props.name);\n",
        "    printf(\"Compute Capability:   %d.%d\\n\", props.major, props.minor);\n",
        "\n",
        "    printf(\"\\n--- On-Chip Memory (SM Level) ---\\n\");\n",
        "    printf(\"Max Shared Memory per SM (Total L1/SMEM Pool): %zu bytes (%.0f KB)\\n\",\n",
        "           props.sharedMemPerMultiprocessor,\n",
        "           (double)props.sharedMemPerMultiprocessor / 1024.0);\n",
        "\n",
        "    printf(\"Max Shared Memory per Block (Programmable Limit): %zu bytes (%.0f KB)\\n\",\n",
        "           props.sharedMemPerBlock,\n",
        "           (double)props.sharedMemPerBlock / 1024.0);\n",
        "\n",
        "    printf(\"\\n--- Off-Chip / Global Memory ---\\n\");\n",
        "\n",
        "    printf(\"Total Global Memory: %zu bytes (%.2f GB)\\n\",\n",
        "           props.totalGlobalMem,\n",
        "           (double)props.totalGlobalMem / (1024.0 * 1024.0 * 1024.0));\n",
        "\n",
        "    printf(\"L2 Cache Size: %d bytes (%.0f KB / %.1f MB)\\n\",\n",
        "           props.l2CacheSize,\n",
        "           (double)props.l2CacheSize / 1024.0,\n",
        "           (double)props.l2CacheSize / (1024.0 * 1024.0));\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAwl6HzCokU9",
        "outputId": "532224cc-1f42-4bc6-c070-16c45b67a4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting basic_info.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc basic_info.cu -o basic_info -gencode arch=compute_75,code=sm_75"
      ],
      "metadata": {
        "id": "iAMgA-bpozIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./basic_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu_dTxJFozTt",
        "outputId": "c72b4cba-0117-4ceb-8355-fbe648bb2be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GPU Device: Tesla T4 ---\n",
            "Compute Capability:   7.5\n",
            "\n",
            "--- On-Chip Memory (SM Level) ---\n",
            "Max Shared Memory per SM (Total L1/SMEM Pool): 65536 bytes (64 KB)\n",
            "Max Shared Memory per Block (Programmable Limit): 49152 bytes (48 KB)\n",
            "\n",
            "--- Off-Chip / Global Memory ---\n",
            "Total Global Memory: 15828320256 bytes (14.74 GB)\n",
            "L2 Cache Size: 4194304 bytes (4096 KB / 4.0 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습 1. Vector Addition\n",
        "\n",
        "*   기본 실습 (데이터 할당, 이동, warp와 thread block 크기 설정, 커널 코드 작성)\n",
        "*   하나의 쓰레드가 하나의 연산을 처리함을 가정\n",
        "*   2D-vector addition으로 확장"
      ],
      "metadata": {
        "id": "OijGVLkvRAaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_1D_addition.cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "void initializeVectors(int* a, int* b, int size) {\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        a[i] = rand() % 100;\n",
        "        b[i] = rand() % 100;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void vectorAdd(const int* a, const int* b, int* c, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "      c[idx] = a[idx] + b[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "bool verifyResult(int* vector_a, int* vector_b, int* result, int size) {\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        if (result[i] != vector_a[i] + vector_b[i]) {\n",
        "            return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "\n",
        "    int N = (argc > 1) ? atoi(argv[1]) : 1<<20;\n",
        "\n",
        "    int* vector_a, *vector_b, *result_vector;\n",
        "\n",
        "    vector_a = (int*)malloc(sizeof(int) * N);\n",
        "    vector_b = (int*)malloc(sizeof(int) * N);\n",
        "    result_vector = (int*)malloc(sizeof(int) * N);\n",
        "\n",
        "    int* d_a, * d_b, * d_c;\n",
        "    cudaMalloc(&d_a, sizeof(int) * N);\n",
        "    cudaMalloc(&d_b, sizeof(int) * N);\n",
        "    cudaMalloc(&d_c, sizeof(int) * N);\n",
        "\n",
        "    initializeVectors(vector_a, vector_b, N);\n",
        "    printf(\"Vectors initialized with %d elements each.\\n\", N);\n",
        "    cudaMemcpy(d_a, vector_a, sizeof(int) * N, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, vector_b, sizeof(int) * N, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "\n",
        "    int blockSize = (argc > 2) ? atoi(argv[2]) : 256;\n",
        "    printf(\"Using block size of %d.\\n\", blockSize);\n",
        "\n",
        "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "    printf(\"Launching kernel with %d blocks of size %d.\\n\", numBlocks, blockSize);\n",
        "\n",
        "    vectorAdd<<<numBlocks, blockSize>>>(d_a, d_b, d_c, N);\n",
        "\n",
        "    cudaMemcpy(result_vector, d_c, sizeof(int) * N, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "    if (verifyResult(vector_a, vector_b, result_vector, N)) {\n",
        "        printf(\"Results are correct!\\n\");\n",
        "    } else {\n",
        "        printf(\"Results are incorrect!\\n\");\n",
        "    }\n",
        "\n",
        "    free(vector_a);\n",
        "    free(vector_b);\n",
        "    free(result_vector);\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8SswuUQRpB-",
        "outputId": "bef320bd-a9cf-4593-9e17-d201cb3a828c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vector_1D_addition.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vector_1D_addition.cu -o vector_1D_addition -gencode arch=compute_75,code=sm_75"
      ],
      "metadata": {
        "id": "u_mJCQdWTx3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./vector_1D_addition 65536 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZyDEPg8T2Lx",
        "outputId": "42461f40-ebc6-40c7-a13d-f086b8b16594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectors initialized with 65536 elements each.\n",
            "Using block size of 16.\n",
            "Launching kernel with 4096 blocks of size 16.\n",
            "Results are correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_2D_addition.cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "__global__ void matrixAdd(const int* a, const int* b, int* c, int width, int height) {\n",
        "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    if (col < width && row < height) {\n",
        "        int idx = row * width + col;\n",
        "        c[idx] = a[idx] + b[idx];\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "bool verifyResult(int* matrix_a, int* matrix_b, int* result, int width, int height) {\n",
        "    for (int i = 0; i < width * height; i++) {\n",
        "        if (result[i] != matrix_a[i] + matrix_b[i]) {\n",
        "            return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "void initializeMatrix(int* a, int* b, int width, int height) {\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i < width * height; i++) {\n",
        "        a[i] = rand() % 100;\n",
        "        b[i] = rand() % 100;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int width = (argc > 1) ? atoi(argv[1]) : 1024;\n",
        "    int height = (argc > 2) ? atoi(argv[2]) : 1024;\n",
        "    int total_elements = width * height;\n",
        "\n",
        "    int* matrix_a, *matrix_b, *result_matrix;\n",
        "\n",
        "    matrix_a = (int*)malloc(sizeof(int) * total_elements);\n",
        "    matrix_b = (int*)malloc(sizeof(int) * total_elements);\n",
        "    result_matrix = (int*)malloc(sizeof(int) * total_elements);\n",
        "\n",
        "    int* d_a, * d_b, * d_c;\n",
        "\n",
        "    cudaMalloc(&d_a, sizeof(int) * total_elements);\n",
        "    cudaMalloc(&d_b, sizeof(int) * total_elements);\n",
        "    cudaMalloc(&d_c, sizeof(int) * total_elements);\n",
        "\n",
        "    initializeMatrix(matrix_a, matrix_b, width, height);\n",
        "    printf(\"Matrix initialized with size %d x %d (%d elements).\\n\", width, height, total_elements);\n",
        "\n",
        "    cudaMemcpy(d_a, matrix_a, sizeof(int) * total_elements, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, matrix_b, sizeof(int) * total_elements, cudaMemcpyHostToDevice);\n",
        "\n",
        "    int blockSide = (argc > 3) ? atoi(argv[3]) : 16;\n",
        "\n",
        "    dim3 blockSize(blockSide, blockSide);\n",
        "    dim3 gridSize((width + blockSize.x - 1) / blockSize.x,\n",
        "                  (height + blockSize.y - 1) / blockSize.y);\n",
        "\n",
        "    printf(\"Launching kernel with Grid(%d, %d) and Block(%d, %d).\\n\",\n",
        "           gridSize.x, gridSize.y, blockSize.x, blockSize.y);\n",
        "\n",
        "    matrixAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, width, height);\n",
        "\n",
        "    cudaMemcpy(result_matrix, d_c, sizeof(int) * total_elements, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    if (verifyResult(matrix_a, matrix_b, result_matrix, width, height)) {\n",
        "        printf(\"Results are correct!\\n\");\n",
        "    } else {\n",
        "        printf(\"Results are incorrect!\\n\");\n",
        "    }\n",
        "\n",
        "    free(matrix_a);\n",
        "    free(matrix_b);\n",
        "    free(result_matrix);\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0k5Um2CW4pp",
        "outputId": "79cc1f6f-f3f9-4d7a-f423-61a7e0abfc22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vector_2D_addition.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vector_2D_addition.cu -o vector_2D_addition -gencode arch=compute_75,code=sm_75"
      ],
      "metadata": {
        "id": "fYBcL4_beHBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./vector_2D_addition 2048 2048 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzVpv-s7eJv2",
        "outputId": "673f08b6-633e-4e78-e163-001bf5077482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix initialized with size 2048 x 2048 (4194304 elements).\n",
            "Launching kernel with Grid(128, 128) and Block(16, 16).\n",
            "Results are correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습 2. General Matrix-Matrix Multiplication (GEMM)\n",
        "\n",
        "*   기본 행렬 곱 연산\n",
        "*   coalesced 메모리 접근 방식\n",
        "*   공유 메모리(shared memory)를 활용한 최적화\n",
        "*   행렬 크기 각 요소가 적당히 크다는 것을 가정 (General)\n",
        "\n",
        "*주의: 수행시간 측정 시, 처음 실행은 다소 느릴 수도 있기 때문에 반복 수행하여 수행 시간을 측정할 것*"
      ],
      "metadata": {
        "id": "ovcUKytmeSXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile naive_gemm.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "__global__ void gemm_naive_kernel(const int* A, const int* B, int* C, int M, int N, int K) {\n",
        "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (row >= M || col >= N) return;\n",
        "    int C_val = 0;\n",
        "\n",
        "    for (int k = 0; k < K; ++k) {\n",
        "        C_val += A[row * K + k] * B[k * N + col];\n",
        "    }\n",
        "\n",
        "    C[row * N + col] = C_val;\n",
        "}\n",
        "\n",
        "void gemm_cpu(const int* A, const int* B, int* C, int M, int N, int K) {\n",
        "    for (int i = 0; i < M; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            int C_val = 0;\n",
        "            for (int k = 0; k < K; ++k) {\n",
        "                C_val += A[i * K + k] * B[k * N + j];\n",
        "            }\n",
        "            C[i * N + j] = C_val;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void initializeMatrix(int* matrix, int size) {\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        matrix[i] = rand() % 100;\n",
        "    }\n",
        "}\n",
        "\n",
        "bool verifyResult(const int* C_gpu, const int* C_cpu, int M, int N) {\n",
        "    for (int i = 0; i < M * N; ++i) {\n",
        "        if (C_gpu[i] != C_cpu[i]) {\n",
        "            printf(\"Error at index %d: GPU=%d, CPU=%d\\n\", i, C_gpu[i], C_cpu[i]);\n",
        "            return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    if (argc != 5) {\n",
        "        fprintf(stderr, \"usage: %s <M> <N> <K> <num_thread>\\n\", argv[0]);\n",
        "        fprintf(stderr, \"  <num_thread>: Block size (e.g., 16 for 16x16 block)\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int M = atoi(argv[1]);\n",
        "    int N = atoi(argv[2]);\n",
        "    int K = atoi(argv[3]);\n",
        "    int num_thread = atoi(argv[4]);\n",
        "\n",
        "    if (M <= 0 || N <= 0 || K <= 0 || num_thread <= 0) {\n",
        "        fprintf(stderr, \"M, N, K, num_thread must be greater than 0.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    printf(\"Executing GEMM C(M,N) = A(M,K) * B(K,N)\\n\");\n",
        "    printf(\"M=%d, N=%d, K=%d\\n\", M, N, K);\n",
        "    printf(\"Block dimensions: %d x %d (Total %d threads/block)\\n\", num_thread, num_thread, num_thread * num_thread);\n",
        "\n",
        "    size_t A_size = (size_t)M * K * sizeof(int);\n",
        "    size_t B_size = (size_t)K * N * sizeof(int);\n",
        "    size_t C_size = (size_t)M * N * sizeof(int);\n",
        "\n",
        "    int* h_A = (int*)malloc(A_size);\n",
        "    int* h_B = (int*)malloc(B_size);\n",
        "    int* h_C_gpu = (int*)malloc(C_size);\n",
        "    int* h_C_cpu = (int*)malloc(C_size);\n",
        "\n",
        "    srand(123);\n",
        "    initializeMatrix(h_A, M * K);\n",
        "    initializeMatrix(h_B, K * N);\n",
        "\n",
        "    int *d_A, *d_B, *d_C;\n",
        "    cudaMalloc(&d_A, A_size);\n",
        "    cudaMalloc(&d_B, B_size);\n",
        "    cudaMalloc(&d_C, C_size);\n",
        "\n",
        "    cudaMemcpy(d_A, h_A, A_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, B_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    dim3 blockDim(num_thread, num_thread);\n",
        "    dim3 gridDim((M + blockDim.x - 1) / blockDim.x,\n",
        "                   (N + blockDim.y - 1) / blockDim.y);\n",
        "\n",
        "    printf(\"Launching Kernel: gridDim(%d, %d), blockDim(%d, %d)\\n\", gridDim.x, gridDim.y, blockDim.x, blockDim.y);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    gemm_naive_kernel<<<gridDim, blockDim>>>(d_A, d_B, d_C, M, N, K);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\n",
        "    printf(\"\\n--- Naive Kernel Execution Time --- \\n\");\n",
        "    printf(\"Time: %.4f ms\\n\", milliseconds);\n",
        "\n",
        "    cudaMemcpy(h_C_gpu, d_C, C_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"\\nVerifying results...\\n\");\n",
        "    //gemm_cpu(h_A, h_B, h_C_cpu, M, N, K);\n",
        "\n",
        "    if (verifyResult(h_C_gpu, h_C_cpu, M, N)) {\n",
        "       printf(\"Success: Results are correct!\\n\");\n",
        "    } else {\n",
        "        printf(\"Failure: Results are incorrect!\\n\");\n",
        "    }\n",
        "\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C_gpu);\n",
        "    free(h_C_cpu);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7qC-M4KgBM1",
        "outputId": "ebab5828-20f0-435b-8db1-d9de545de03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting naive_gemm.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc naive_gemm.cu -o naive_gemm -gencode arch=compute_75,code=sm_75"
      ],
      "metadata": {
        "id": "Xcu4O1Q_g22b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./naive_gemm 2048 2048 1024 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvCzP09NhBvq",
        "outputId": "b85763bf-32f7-462f-c2b4-eb5bc7eba20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing GEMM C(M,N) = A(M,K) * B(K,N)\n",
            "M=2048, N=2048, K=1024\n",
            "Block dimensions: 16 x 16 (Total 256 threads/block)\n",
            "Launching Kernel: gridDim(128, 128), blockDim(16, 16)\n",
            "\n",
            "--- Naive Kernel Execution Time --- \n",
            "Time: 169.6749 ms\n",
            "\n",
            "Verifying results...\n",
            "Error at index 0: GPU=2597831, CPU=0\n",
            "Failure: Results are incorrect!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./naive_gemm 1024 1024 1024 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM_1G3D9n9MV",
        "outputId": "32ebbf55-d0e3-4d27-9ecb-9e33447b5892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing GEMM C(M,N) = A(M,K) * B(K,N)\n",
            "M=1024, N=1024, K=1024\n",
            "Block dimensions: 32 x 32 (Total 1024 threads/block)\n",
            "Launching Kernel: gridDim(32, 32), blockDim(32, 32)\n",
            "\n",
            "--- Naive Kernel Execution Time --- \n",
            "Time: 58.7607 ms\n",
            "\n",
            "Verifying results...\n",
            "Error at index 0: GPU=2667833, CPU=0\n",
            "Failure: Results are incorrect!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile coalesced_gemm.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "__global__ void gemm_coalesced_kernel(const int* A, const int* B, int* C, int M, int N, int K) {\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (row >= M || col >= N) return;\n",
        "    int C_val = 0;\n",
        "\n",
        "    for (int k = 0; k < K; ++k) {\n",
        "        C_val += A[row * K + k] * B[k * N + col];\n",
        "    }\n",
        "\n",
        "    C[row * N + col] = C_val;\n",
        "}\n",
        "\n",
        "void gemm_cpu(const int* A, const int* B, int* C, int M, int N, int K) {\n",
        "    for (int i = 0; i < M; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            int C_val = 0;\n",
        "            for (int k = 0; k < K; ++k) {\n",
        "                C_val += A[i * K + k] * B[k * N + j];\n",
        "            }\n",
        "            C[i * N + j] = C_val;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void initializeMatrix(int* matrix, int size) {\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        matrix[i] = rand() % 10;\n",
        "    }\n",
        "}\n",
        "\n",
        "bool verifyResult(const int* C_gpu, const int* C_cpu, int M, int N) {\n",
        "    for (int i = 0; i < M * N; ++i) {\n",
        "        if (C_gpu[i] != C_cpu[i]) {\n",
        "            printf(\"Error at index %d: GPU=%d, CPU=%d\\n\", i, C_gpu[i], C_cpu[i]);\n",
        "            return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    if (argc != 5) {\n",
        "        fprintf(stderr, \"사용법: %s <M> <N> <K> <num_thread>\\n\", argv[0]);\n",
        "        fprintf(stderr, \"  <num_thread>: 블록의 한 변 크기 (예: 16이면 16x16 블록)\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int M = atoi(argv[1]);\n",
        "    int N = atoi(argv[2]);\n",
        "    int K = atoi(argv[3]);\n",
        "    int num_thread = atoi(argv[4]);\n",
        "\n",
        "    if (M <= 0 || N <= 0 || K <= 0 || num_thread <= 0) {\n",
        "        fprintf(stderr, \"M, N, K, num_thread는 0보다 커야 합니다.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    printf(\"Executing GEMM C(M,N) = A(M,K) * B(K,N)\\n\");\n",
        "    printf(\"M=%d, N=%d, K=%d\\n\", M, N, K);\n",
        "    printf(\"Block dimensions: %d x %d (Total %d threads/block)\\n\", num_thread, num_thread, num_thread * num_thread);\n",
        "\n",
        "    size_t A_size = (size_t)M * K * sizeof(int);\n",
        "    size_t B_size = (size_t)K * N * sizeof(int);\n",
        "    size_t C_size = (size_t)M * N * sizeof(int);\n",
        "\n",
        "    int* h_A = (int*)malloc(A_size);\n",
        "    int* h_B = (int*)malloc(B_size);\n",
        "    int* h_C_gpu = (int*)malloc(C_size);\n",
        "    int* h_C_cpu = (int*)malloc(C_size);\n",
        "\n",
        "    srand(123);\n",
        "    initializeMatrix(h_A, M * K);\n",
        "    initializeMatrix(h_B, K * N);\n",
        "\n",
        "    int *d_A, *d_B, *d_C;\n",
        "    cudaMalloc(&d_A, A_size);\n",
        "    cudaMalloc(&d_B, B_size);\n",
        "    cudaMalloc(&d_C, C_size);\n",
        "\n",
        "    cudaMemcpy(d_A, h_A, A_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, B_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    dim3 blockDim(num_thread, num_thread);\n",
        "    dim3 gridDim((N + blockDim.x - 1) / blockDim.x,\n",
        "                   (M + blockDim.y - 1) / blockDim.y);\n",
        "\n",
        "    printf(\"Launching Kernel: gridDim(%d, %d), blockDim(%d, %d)\\n\", gridDim.x, gridDim.y, blockDim.x, blockDim.y);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    gemm_coalesced_kernel<<<gridDim, blockDim>>>(d_A, d_B, d_C, M, N, K);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"\\n--- Coalesced Kernel Execution Time --- \\n\");\n",
        "    printf(\"Time: %.4f ms\\n\", milliseconds);\n",
        "\n",
        "    cudaMemcpy(h_C_gpu, d_C, C_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"\\nVerifying results...\\n\");\n",
        "    //gemm_cpu(h_A, h_B, h_C_cpu, M, N, K);\n",
        "\n",
        "    if (verifyResult(h_C_gpu, h_C_cpu, M, N)) {\n",
        "        printf(\"Success: Results are correct!\\n\");\n",
        "    } else {\n",
        "        printf(\"Failure: Results are incorrect!\\n\");\n",
        "    }\n",
        "\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C_gpu);\n",
        "    free(h_C_cpu);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bONwIVrUhcnl",
        "outputId": "39ebaafb-27f5-4f1a-d03e-49df02d05b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting coalesced_gemm.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc coalesced_gemm.cu -o coalesced_gemm -gencode arch=compute_75,code=sm_75"
      ],
      "metadata": {
        "id": "w_6gCJ1kiLAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./coalesced_gemm 2048 2048 1024 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7bfkYGJiQXV",
        "outputId": "c56bf59c-1484-48ef-f65a-2df332e08094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing GEMM C(M,N) = A(M,K) * B(K,N)\n",
            "M=2048, N=2048, K=1024\n",
            "Block dimensions: 16 x 16 (Total 256 threads/block)\n",
            "Launching Kernel: gridDim(128, 128), blockDim(16, 16)\n",
            "\n",
            "--- Coalesced Kernel Execution Time --- \n",
            "Time: 22.8213 ms\n",
            "\n",
            "Verifying results...\n",
            "Error at index 0: GPU=21271, CPU=0\n",
            "Failure: Results are incorrect!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./coalesced_gemm 1024 1024 1024 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i5vIlCIiY4G",
        "outputId": "83173f78-e5d7-4027-cc33-13697e056666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing GEMM C(M,N) = A(M,K) * B(K,N)\n",
            "M=1024, N=1024, K=1024\n",
            "Block dimensions: 32 x 32 (Total 1024 threads/block)\n",
            "Launching Kernel: gridDim(32, 32), blockDim(32, 32)\n",
            "\n",
            "--- Coalesced Kernel Execution Time --- \n",
            "Time: 6.8060 ms\n",
            "\n",
            "Verifying results...\n",
            "Error at index 0: GPU=21853, CPU=0\n",
            "Failure: Results are incorrect!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile shared_memory_gemm.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "__global__ void gemm_smem_dynamic_kernel(const int* A, const int* B, int* C, int M, int N, int K) {\n",
        "\n",
        "    extern __shared__ int s_data[];//max 48KB\n",
        "\n",
        "    const int TILE_DIM = blockDim.x;\n",
        "\n",
        "    int* s_A = s_data;\n",
        "    int* s_B = (int*)&s_A[TILE_DIM * TILE_DIM];\n",
        "\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "\n",
        "    int col = blockIdx.x * TILE_DIM + tx;\n",
        "    int row = blockIdx.y * TILE_DIM + ty;\n",
        "    int C_val = 0;\n",
        "\n",
        "    for (int p = 0; p < (K + TILE_DIM - 1) / TILE_DIM; ++p) {\n",
        "        int a_load_col = p * TILE_DIM + tx;\n",
        "        int a_load_row = row;\n",
        "        if (a_load_row < M && a_load_col < K) {\n",
        "            s_A[ty * TILE_DIM + tx] = A[a_load_row * K + a_load_col];\n",
        "        } else {\n",
        "            s_A[ty * TILE_DIM + tx] = 0;\n",
        "        }\n",
        "    int b_load_col = col;\n",
        "    int b_load_row = p * TILE_DIM + ty;\n",
        "    if (b_load_row < K && b_load_col < N) {\n",
        "        s_B[ty * TILE_DIM + tx] = B[b_load_row * N + b_load_col];\n",
        "    } else {\n",
        "        s_B[ty * TILE_DIM + tx] = 0;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "    for (int k_tile = 0; k_tile < TILE_DIM; ++k_tile) {\n",
        "        C_val += s_A[ty * TILE_DIM + k_tile] * s_B[k_tile * TILE_DIM + tx];\n",
        "    }\n",
        "    __syncthreads();\n",
        "}\n",
        "if (row < M && col < N) {\n",
        "    C[row * N + col] = C_val;\n",
        "}\n",
        "}\n",
        "\n",
        "void gemm_cpu(const int* A, const int* B, int* C, int M, int N, int K) {\n",
        "    for (int i = 0; i < M; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            int C_val = 0;\n",
        "            for (int k = 0; k < K; ++k) {\n",
        "                C_val += A[i * K + k] * B[k * N + j];\n",
        "            }\n",
        "            C[i * N + j] = C_val;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void initializeMatrix(int* matrix, int size) {\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        matrix[i] = rand() % 10;\n",
        "    }\n",
        "}\n",
        "\n",
        "bool verifyResult(const int* C_gpu, const int* C_cpu, int M, int N) {\n",
        "    for (int i = 0; i < M * N; ++i) {\n",
        "        if (C_gpu[i] != C_cpu[i]) {\n",
        "            printf(\"Error at index %d: GPU=%d, CPU=%d\\n\", i, C_gpu[i], C_cpu[i]);\n",
        "            return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    if (argc != 5) {\n",
        "        fprintf(stderr, \"사용법: %s <M> <N> <K> <num_thread>\\n\", argv[0]);\n",
        "        fprintf(stderr, \"  <num_thread>: 타일 및 블록의 한 변 크기 (예: 16 또는 32)\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int M = atoi(argv[1]);\n",
        "    int N = atoi(argv[2]);\n",
        "    int K = atoi(argv[3]);\n",
        "    int num_thread = atoi(argv[4]); // TILE_DIM\n",
        "\n",
        "    if (M <= 0 || N <= 0 || K <= 0 || num_thread <= 0) {\n",
        "        fprintf(stderr, \"M, N, K, num_thread는 0보다 커야 합니다.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    printf(\"Executing Dynamic Tiled GEMM C(M,N) = A(M,K) * B(K,N)\\n\");\n",
        "    printf(\"M=%d, N=%d, K=%d\\n\", M, N, K);\n",
        "    printf(\"TILE_DIM: %d x %d (Total %d threads/block)\\n\", num_thread, num_thread, num_thread * num_thread);\n",
        "\n",
        "    size_t A_size = (size_t)M * K * sizeof(int);\n",
        "    size_t B_size = (size_t)K * N * sizeof(int);\n",
        "    size_t C_size = (size_t)M * N * sizeof(int);\n",
        "\n",
        "    int* h_A = (int*)malloc(A_size);\n",
        "    int* h_B = (int*)malloc(B_size);\n",
        "    int* h_C_gpu = (int*)malloc(C_size);\n",
        "    int* h_C_cpu = (int*)malloc(C_size);\n",
        "\n",
        "    srand(123);\n",
        "    initializeMatrix(h_A, M * K);\n",
        "    initializeMatrix(h_B, K * N);\n",
        "\n",
        "    int *d_A, *d_B, *d_C;\n",
        "    cudaMalloc(&d_A, A_size);\n",
        "    cudaMalloc(&d_B, B_size);\n",
        "    cudaMalloc(&d_C, C_size);\n",
        "\n",
        "    cudaMemcpy(d_A, h_A, A_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, B_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    dim3 blockDim(num_thread, num_thread);\n",
        "    dim3 gridDim((N + blockDim.x - 1) / blockDim.x,\n",
        "                   (M + blockDim.y - 1) / blockDim.y);\n",
        "\n",
        "    size_t shared_mem_bytes = 2 * (size_t)num_thread * num_thread * sizeof(int);\n",
        "\n",
        "    printf(\"Launching Kernel: gridDim(%d, %d), blockDim(%d, %d), sharedMem= %zu bytes\\n\",\n",
        "           gridDim.x, gridDim.y, blockDim.x, blockDim.y, shared_mem_bytes);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    gemm_smem_dynamic_kernel<<<gridDim, blockDim, shared_mem_bytes>>>(d_A, d_B, d_C, M, N, K);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"\\n--- Dynamic SMEM Kernel Execution Time --- \\n\");\n",
        "    printf(\"Time: %.4f ms\\n\", milliseconds);\n",
        "\n",
        "    cudaMemcpy(h_C_gpu, d_C, C_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"\\nVerifying results...\\n\");\n",
        "    //gemm_cpu(h_A, h_B, h_C_cpu, M, N, K);\n",
        "\n",
        "    if (verifyResult(h_C_gpu, h_C_cpu, M, N)) {\n",
        "       printf(\"Success: Results are correct!\\n\");\n",
        "    } else {\n",
        "       printf(\"Failure: Results are incorrect!\\n\");\n",
        "    }\n",
        "\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C_gpu);\n",
        "    free(h_C_cpu);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaUCHrz4imDe",
        "outputId": "cf802a82-f073-477b-a109-72d41d95e0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting shared_memory_gemm.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc shared_memory_gemm.cu -o shared_memory_gemm -gencode arch=compute_75,code=sm_75"
      ],
      "metadata": {
        "id": "TaI3OGlkkuQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./shared_memory_gemm 1024 1024 512 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Swcf0Hbk1F2",
        "outputId": "86b7d9aa-1cc1-430d-b14f-6966ada7fa83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing Dynamic Tiled GEMM C(M,N) = A(M,K) * B(K,N)\n",
            "M=1024, N=1024, K=512\n",
            "TILE_DIM: 16 x 16 (Total 256 threads/block)\n",
            "Launching Kernel: gridDim(64, 64), blockDim(16, 16), sharedMem= 2048 bytes\n",
            "\n",
            "--- Dynamic SMEM Kernel Execution Time --- \n",
            "Time: 3.8003 ms\n",
            "\n",
            "Verifying results...\n",
            "Error at index 0: GPU=10789, CPU=0\n",
            "Failure: Results are incorrect!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./shared_memory_gemm 2048 2048 1024 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMFVGw6vkxES",
        "outputId": "eaeeef90-a0d7-4f8d-93df-319ede9271b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing Dynamic Tiled GEMM C(M,N) = A(M,K) * B(K,N)\n",
            "M=2048, N=2048, K=1024\n",
            "TILE_DIM: 32 x 32 (Total 1024 threads/block)\n",
            "Launching Kernel: gridDim(64, 64), blockDim(32, 32), sharedMem= 8192 bytes\n",
            "\n",
            "--- Dynamic SMEM Kernel Execution Time --- \n",
            "Time: 28.5811 ms\n",
            "\n",
            "Verifying results...\n",
            "Error at index 0: GPU=21271, CPU=0\n",
            "Failure: Results are incorrect!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./coalesced_gemm 4096 4096 4096 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXnpfelZJwQw",
        "outputId": "847f1cbc-987f-4e58-cbf3-d79bf801da61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing GEMM C(M,N) = A(M,K) * B(K,N)\n",
            "M=4096, N=4096, K=4096\n",
            "Block dimensions: 32 x 32 (Total 1024 threads/block)\n",
            "Launching Kernel: gridDim(128, 128), blockDim(32, 32)\n",
            "\n",
            "--- Coalesced Kernel Execution Time --- \n",
            "Time: 324.3971 ms\n",
            "\n",
            "Verifying results...\n",
            "Error at index 0: GPU=83102, CPU=0\n",
            "Failure: Results are incorrect!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습 2 보고서(해당 칸에 다음의 내용을 기재하시오)\n",
        "\n",
        "1. naive, coalesced에 대해 아래 수행시간 표(ms)와 메모리 접근 패턴 비교 표를 채워넣을 것. (팁: 수행시간 측정 시, host 수행 및 검증 코드는 주석 처리)\n",
        "(숫자와 '|' 표시를 활용)\n",
        "*   thread block size: (16,16)\n",
        "N,M = 1024 | 1024 | 2048 | 4096 | 8192\n",
        "-|-|-|-|-\n",
        "naive | 48.97ms | 174.1ms | 466.25ms | 1345.4ms\n",
        "coalesced | 9.28ms | 37.46ms | 140.7ms | 403.12ms\n",
        "\n",
        "*   thread block size: (32,32)\n",
        "N,M = 4096 | 4096 | 8192 | 16384 | 32768\n",
        "-|-|-|-|-|\n",
        "naive | 750.63ms | 2391.87ms | 9051.92ms | 35807.01ms\n",
        "coalesced | 107.33ms | 298.29ms| 989.71ms | 3577.96ms\n",
        "\n",
        "*   warp 당 메모리 접근 패턴 비교 (세부 사항은 강의자료 참고)\n",
        "\n",
        "warp 배치 방식 | 행렬 A | 행렬 B | 행렬 C | 메모리 전송 수\n",
        "-|-|-|-|-\n",
        "세로 배치 | Non-coalesced | Broadcast | Non-coalesced | 33 x K + 32\n",
        "가로 배치 | Broadcast | Coalesced | Coalesced |K+2\n",
        "\n",
        "2. coalesced와 smem에 대해 아래 수행 시간(ms) 표를 채워넣고 간단히 분석할 것\n",
        "*   thread block size: (16,16)\n",
        "N,M = 1024 | 256 | 512 | 1024 | 2048  \n",
        "-|-|-|-|-\n",
        "coalesced | 0.82ms | 2.93ms | 15.32ms | 109.57ms\n",
        "smem | 0.61ms | 2.04ms | 10.18ms | 74.86ms\n",
        "\n",
        "SMEM 방법이 coalesced 대비 약 1.4배 정도 빠릅니다. 크기가 작은 행렬에서는 차이가 적지만 큰 행렬에서는 성능 향상이 명확합니다.\n",
        "\n",
        "*   thread block size: (32,32)\n",
        "N,M = 4096 | 4096 | 8192 | 16384 | 32768  \n",
        "-|-|-|-|-\n",
        "coalesced | 317.63ms | 1800.81ms | 15214.06ms | 145342.67ms\n",
        "smem | 199.94ms | 1695.41ms | 13002.36ms | 107589.5ms\n",
        "\n",
        "SMEM 방법이 coalesced 대비 약 1.04배 더 빠릅니다.\n",
        " 32x32 블록 사이즈에서는 성능 향상이 제한적입니다. SMEM 최적화 방법은 전역 메모리 접근 횟수 감소로 타일링을 통해 데이터를 재사용했습니다. 단, 32x32 블록 사이즈에서는 SMEM 할당으로 인해 L1 캐시 공간이 감소하여 선능 향상의 제한이 있었습니다.\n",
        "\n",
        " 결론적으로, SMEM 최적화 방법은 메모리 재사용이 많은 경우 효과적이고 Block 크기와 SMEM 사용량의 균형이 중요하다고 생각합니다. 또한 Coalesced 접근만으로도 충분한 성능을 달성할 수 있다고 생각합니다.\n",
        "\n",
        "Prefetching이라고 다음 타일 데이터를 실행 중에 미리 로하여 메모리 대기 시간을 감소하면 더 빠른 성능을 낼 수 있다고 생각합니다."
      ],
      "metadata": {
        "id": "e-MzrDohqxDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Convolution 연산 구현\n"
      ],
      "metadata": {
        "id": "i9x7YvRJ01qK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile naive_convolution.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#define BLOCK_DIM 32\n",
        "\n",
        "__global__ void conv_naive_kernel(const int* input, const int* kernel, int* output,\n",
        "                                  int M, int N, int K) {\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (row >= M || col >= N) return;\n",
        "    int k_radius = K / 2;\n",
        "    int output_val = 0;\n",
        "    for (int i = -k_radius; i <= k_radius; ++i) {\n",
        "        for (int j = -k_radius; j <= k_radius; ++j) {\n",
        "            int in_row = row + i;\n",
        "            int in_col = col + j;\n",
        "            if (in_row >= 0 && in_row < M && in_col >= 0 && in_col < N) {\n",
        "                int k_row = i + k_radius;\n",
        "                int k_col = j + k_radius;\n",
        "                output_val += input[in_row * N + in_col] * kernel[k_row * K + k_col];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    output[row * N + col] = output_val;\n",
        "\n",
        "}\n",
        "\n",
        "void conv_cpu(const int* input, const int* kernel, int* output, int M, int N, int K) {\n",
        "    int k_radius = K / 2;\n",
        "\n",
        "    for (int row = 0; row < M; ++row) {\n",
        "        for (int col = 0; col < N; ++col) {\n",
        "\n",
        "            int output_val = 0;\n",
        "            for (int i = -k_radius; i <= k_radius; ++i) {\n",
        "                for (int j = -k_radius; j <= k_radius; ++j) {\n",
        "\n",
        "                    int in_row = row + i;\n",
        "                    int in_col = col + j;\n",
        "\n",
        "                    if (in_row >= 0 && in_row < M && in_col >= 0 && in_col < N) {\n",
        "                        int k_row = i + k_radius;\n",
        "\n",
        "                        int k_col = j + k_radius;\n",
        "                        output_val += input[in_row * N + in_col] * kernel[k_row * K + k_col];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            output[row * N + col] = output_val;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void initializeMatrix(int* matrix, int size) {\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        matrix[i] = rand() % 10;\n",
        "    }\n",
        "}\n",
        "\n",
        "bool verifyResult(const int* C_gpu, const int* C_cpu, int M, int N) {\n",
        "    for (int i = 0; i < M * N; ++i) {\n",
        "        if (C_gpu[i] != C_cpu[i]) {\n",
        "            printf(\"Error at index %d: GPU=%d, CPU=%d\\n\", i, C_gpu[i], C_cpu[i]);\n",
        "            return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    if (argc != 4) {\n",
        "        fprintf(stderr, \"usage: %s <M> <N> <K_dim>\\n\", argv[0]);\n",
        "        fprintf(stderr, \"  <M>: height\\n\");\n",
        "        fprintf(stderr, \"  <N>: width\\n\");\n",
        "        fprintf(stderr, \"  <K_dim>: kernel size (odd, e.g., 3, 5, 7)\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int M = atoi(argv[1]);\n",
        "    int N = atoi(argv[2]);\n",
        "    int K = atoi(argv[3]);\n",
        "\n",
        "    if (M <= 0 || N <= 0 || K <= 0) {\n",
        "        fprintf(stderr, \"M, N, K_dim must be bigger than 0.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "    if (K % 2 == 0) {\n",
        "        fprintf(stderr, \"Kernel size (K_dim) must be odd.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    printf(\"Executing 2D Convolution\\n\");\n",
        "    printf(\"Image (M,N): (%d, %d), Kernel (K,K): (%d, %d)\\n\", M, N, K, K);\n",
        "\n",
        "    size_t input_size = (size_t)M * N * sizeof(int);\n",
        "    size_t kernel_size = (size_t)K * K * sizeof(int);\n",
        "    size_t output_size = (size_t)M * N * sizeof(int);\n",
        "\n",
        "    int* h_input = (int*)malloc(input_size);\n",
        "    int* h_kernel = (int*)malloc(kernel_size);\n",
        "    int* h_output_gpu = (int*)malloc(output_size);\n",
        "    int* h_output_cpu = (int*)malloc(output_size);\n",
        "\n",
        "    srand(123);\n",
        "    initializeMatrix(h_input, M * N);\n",
        "    initializeMatrix(h_kernel, K * K);\n",
        "\n",
        "    int *d_input, *d_kernel, *d_output;\n",
        "\n",
        "    cudaMalloc(&d_input, input_size);\n",
        "    cudaMalloc(&d_kernel, kernel_size);\n",
        "    cudaMalloc(&d_output, output_size);\n",
        "\n",
        "\n",
        "    cudaMemcpy(d_input, h_input, input_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_kernel, h_kernel, kernel_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    dim3 blockDim(BLOCK_DIM, BLOCK_DIM);\n",
        "    dim3 gridDim((N + blockDim.x - 1) / blockDim.x,\n",
        "                   (M + blockDim.y - 1) / blockDim.y);\n",
        "\n",
        "    printf(\"Launching Kernel: gridDim(%d, %d), blockDim(%d, %d)\\n\", gridDim.x, gridDim.y, blockDim.x, blockDim.y);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    conv_naive_kernel<<<gridDim, blockDim>>>(d_input, d_kernel, d_output, M, N, K);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"\\n--- Naive Conv Kernel Execution Time --- \\n\");\n",
        "    printf(\"Time: %.4f ms\\n\", milliseconds);\n",
        "\n",
        "    cudaMemcpy(h_output_gpu, d_output, output_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"\\nVerifying results...\\n\");\n",
        "    conv_cpu(h_input, h_kernel, h_output_cpu, M, N, K);\n",
        "\n",
        "    if (verifyResult(h_output_gpu, h_output_cpu, M, N)) {\n",
        "        printf(\"Success: Results are correct!\\n\");\n",
        "    } else {\n",
        "        printf(\"Failure: Results are incorrect!\\n\");\n",
        "    }\n",
        "\n",
        "    free(h_input);\n",
        "    free(h_kernel);\n",
        "    free(h_output_gpu);\n",
        "    free(h_output_cpu);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_kernel);\n",
        "    cudaFree(d_output);\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxbIwJ8E1caW",
        "outputId": "58b9adb0-9997-49f3-d9a1-71568a43a1d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting naive_convolution.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc naive_convolution.cu -o naive_convolution -gencode arch=compute_75,code=sm_75"
      ],
      "metadata": {
        "id": "CxDN2-QG2SKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./naive_convolution 1024 1024 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uyipGH02YAb",
        "outputId": "4fcabc65-d4a9-46ab-9925-607f76cfeb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing 2D Convolution\n",
            "Image (M,N): (1024, 1024), Kernel (K,K): (5, 5)\n",
            "Launching Kernel: gridDim(32, 32), blockDim(32, 32)\n",
            "\n",
            "--- Naive Conv Kernel Execution Time --- \n",
            "Time: 0.2147 ms\n",
            "\n",
            "Verifying results...\n",
            "Success: Results are correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile optimal_convolution.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#define BLOCK_DIM 32\n",
        "#define MAX_K_DIM 15\n",
        "\n",
        "// Constant Memory 선언\n",
        "__constant__ int c_kernel[MAX_K_DIM * MAX_K_DIM];\n",
        "\n",
        "__global__ void conv_smem_kernel(const int* input, int* output,\n",
        "                                 int M, int N, int K) {\n",
        "\n",
        "    extern __shared__ int smem[];\n",
        "\n",
        "    int k_radius = K / 2;\n",
        "    int tile_width = BLOCK_DIM + 2 * k_radius;  // K - 1 = 2 * k_radius\n",
        "\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "\n",
        "    // 현재 블록 입력 데이터의 시작 위치\n",
        "    int row_start = by * BLOCK_DIM - k_radius;\n",
        "    int col_start = bx * BLOCK_DIM - k_radius;\n",
        "\n",
        "    // 전역 메모리에서 공유 메모리로 데이터 이동\n",
        "    for (int i = ty; i < tile_width; i += BLOCK_DIM) {\n",
        "        for (int j = tx; j < tile_width; j += BLOCK_DIM) {\n",
        "            int in_row = row_start + i;\n",
        "            int in_col = col_start + j;\n",
        "\n",
        "            if (in_row >= 0 && in_row < M && in_col >= 0 && in_col < N) {\n",
        "                smem[i * tile_width + j] = input[in_row * N + in_col];\n",
        "            } else {\n",
        "                smem[i * tile_width + j] = 0;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    int col_out = bx * BLOCK_DIM + tx;\n",
        "    int row_out = by * BLOCK_DIM + ty;\n",
        "\n",
        "    if (row_out >= M || col_out >= N) {\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    int output_val = 0;\n",
        "\n",
        "    // Convolution\n",
        "    for (int ki = 0; ki < K; ++ki) {\n",
        "        for (int kj = 0; kj < K; ++kj) {\n",
        "            int smem_row = ty + ki;\n",
        "            int smem_col = tx + kj;\n",
        "\n",
        "            output_val += smem[smem_row * tile_width + smem_col] * c_kernel[ki * K + kj];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    output[row_out * N + col_out] = output_val;\n",
        "}\n",
        "\n",
        "void conv_cpu(const int* input, const int* kernel, int* output, int M, int N, int K) {\n",
        "    int k_radius = K / 2;\n",
        "\n",
        "    for (int row = 0; row < M; ++row) {\n",
        "        for (int col = 0; col < N; ++col) {\n",
        "\n",
        "            int output_val = 0;\n",
        "            for (int i = -k_radius; i <= k_radius; ++i) {\n",
        "                for (int j = -k_radius; j <= k_radius; ++j) {\n",
        "\n",
        "                    int in_row = row + i;\n",
        "                    int in_col = col + j;\n",
        "\n",
        "                    if (in_row >= 0 && in_row < M && in_col >= 0 && in_col < N) {\n",
        "                        int k_row = i + k_radius;\n",
        "                        int k_col = j + k_radius;\n",
        "                        output_val += input[in_row * N + in_col] * kernel[k_row * K + k_col];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            output[row * N + col] = output_val;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void initializeMatrix(int* matrix, int size) {\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        matrix[i] = rand() % 10;\n",
        "    }\n",
        "}\n",
        "\n",
        "bool verifyResult(const int* C_gpu, const int* C_cpu, int M, int N) {\n",
        "    for (int i = 0; i < M * N; ++i) {\n",
        "        if (C_gpu[i] != C_cpu[i]) {\n",
        "            printf(\"Error at index %d: GPU=%d, CPU=%d\\n\", i, C_gpu[i], C_cpu[i]);\n",
        "            return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    if (argc != 4) {\n",
        "        fprintf(stderr, \"usage: %s <M> <N> <K_dim>\\n\", argv[0]);\n",
        "        fprintf(stderr, \"  <M>: height\\n\");\n",
        "        fprintf(stderr, \"  <N>: width\\n\");\n",
        "        fprintf(stderr, \"  <K_dim>: kernel size (odd, e.g., 3, 5, 7)\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int M = atoi(argv[1]);\n",
        "    int N = atoi(argv[2]);\n",
        "    int K = atoi(argv[3]);\n",
        "\n",
        "    if (M <= 0 || N <= 0 || K <= 0) {\n",
        "        fprintf(stderr, \"M, N, K_dim must be bigger than 0.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "    if (K % 2 == 0) {\n",
        "        fprintf(stderr, \"Kernel size (K_dim) must be odd.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    if (K > MAX_K_DIM) {\n",
        "        fprintf(stderr, \"Error: Kernel size %d exceeds MAX_K_DIM %d\\n\", K, MAX_K_DIM);\n",
        "        fprintf(stderr, \"Please increase MAX_K_DIM in the source and recompile.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "\n",
        "    printf(\"Executing 2D Convolution\\n\");\n",
        "    printf(\"Image (M,N): (%d, %d), Kernel (K,K): (%d, %d)\\n\", M, N, K, K);\n",
        "    printf(\"Block DIM: (%d, %d)\\n\", BLOCK_DIM, BLOCK_DIM);\n",
        "\n",
        "    size_t input_size = (size_t)M * N * sizeof(int);\n",
        "    size_t kernel_size = (size_t)K * K * sizeof(int);\n",
        "    size_t output_size = (size_t)M * N * sizeof(int);\n",
        "\n",
        "    int* h_input = (int*)malloc(input_size);\n",
        "    int* h_kernel = (int*)malloc(kernel_size);\n",
        "    int* h_output_gpu = (int*)malloc(output_size);\n",
        "    int* h_output_cpu = (int*)malloc(output_size);\n",
        "\n",
        "    srand(123);\n",
        "    initializeMatrix(h_input, M * N);\n",
        "    initializeMatrix(h_kernel, K * K);\n",
        "\n",
        "    int *d_input, *d_output;\n",
        "\n",
        "    cudaMalloc(&d_input, input_size);\n",
        "    cudaMalloc(&d_output, output_size);\n",
        "\n",
        "    cudaMemcpy(d_input, h_input, input_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    printf(\"Copying kernel to __constant__ memory...\\n\");\n",
        "    cudaMemcpyToSymbol(c_kernel, h_kernel, kernel_size, 0, cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    dim3 blockDim(BLOCK_DIM, BLOCK_DIM);\n",
        "    dim3 gridDim((N + blockDim.x - 1) / blockDim.x,\n",
        "                 (M + blockDim.y - 1) / blockDim.y);\n",
        "\n",
        "    printf(\"Launching Kernel: gridDim(%d, %d), blockDim(%d, %d)\\n\", gridDim.x, gridDim.y, blockDim.x, blockDim.y);\n",
        "\n",
        "    int tile_width = BLOCK_DIM + 2 * (K / 2);\n",
        "    size_t smem_size = (size_t)tile_width * tile_width * sizeof(int);\n",
        "\n",
        "    printf(\"\\nLaunching SMEM Kernel with %zu bytes of dynamic shared memory.\\n\", smem_size);\n",
        "\n",
        "    cudaMemset(d_output, 0, output_size);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    conv_smem_kernel<<<gridDim, blockDim, smem_size>>>(d_input, d_output, M, N, K);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"\\n--- Tiled Conv Kernel (SMEM + Constant) --- \\n\");\n",
        "    printf(\"Time: %.4f ms\\n\", milliseconds);\n",
        "\n",
        "    cudaMemcpy(h_output_gpu, d_output, output_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // CPU verification\n",
        "    printf(\"\\nRunning CPU verification...\\n\");\n",
        "    conv_cpu(h_input, h_kernel, h_output_cpu, M, N, K);\n",
        "\n",
        "    printf(\"\\nVerifying results (SMEM)...\\n\");\n",
        "    if (verifyResult(h_output_gpu, h_output_cpu, M, N)) {\n",
        "        printf(\"Success: SMEM results are correct!\\n\");\n",
        "    } else {\n",
        "        printf(\"Failure: SMEM results are incorrect!\\n\");\n",
        "    }\n",
        "\n",
        "\n",
        "    free(h_input);\n",
        "    free(h_kernel);\n",
        "    free(h_output_gpu);\n",
        "    free(h_output_cpu);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu7hVRCx2bVo",
        "outputId": "cd1edc3e-dbec-435c-e159-68ce587714ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting optimal_convolution.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc optimal_convolution.cu -o optimal_convolution -gencode arch=compute_75,code=sm_75"
      ],
      "metadata": {
        "id": "XhQ9mRcF4Als"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./optimal_convolution 8192 8192 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM5nbLEG4Gg4",
        "outputId": "47722cdf-5e1c-4985-8db7-b468261066ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing 2D Convolution\n",
            "Image (M,N): (8192, 8192), Kernel (K,K): (5, 5)\n",
            "Block DIM: (32, 32)\n",
            "Copying kernel to __constant__ memory...\n",
            "Launching Kernel: gridDim(256, 256), blockDim(32, 32)\n",
            "\n",
            "Launching SMEM Kernel with 5184 bytes of dynamic shared memory.\n",
            "\n",
            "--- Tiled Conv Kernel (SMEM + Constant) --- \n",
            "Time: 13.4056 ms\n",
            "\n",
            "Running CPU verification...\n",
            "\n",
            "Verifying results (SMEM)...\n",
            "Success: SMEM results are correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "보고서\n",
        "\n",
        "*  Shared Memory 타일링: 32x32 크기의 thread block이 출력 데이터를 계산하기 위해 필요한 입력 데이터를 전역 메모리를 공유 메모리로 한 번에 로드합니다. 이를 통해 동일한 입력 픽셀을 중복해서 읽는 전역 메모리 접근 횟수를 줄였습니다.\n",
        "\n",
        "*   경계 영역 포함 로딩: 컨볼루션 반경(k_radius)으로 인해 타일 외부의 데이터가 연산에 필요합니다. 이는 thread block 크기보다 큰 tile_width(BLOCK_DIM+2 x k_Radius)를 설정하여 주변 경계 영역까지 공유 메모리에 함께 로드했습니다.\n",
        "\n",
        "*   Constant Memory 활용: 연산 중 값이 변하지 않고 모든 thread가 동시에 접근하는 커널 가중치를 constant 메모리 커널에 할당했습니다. 이는 상수 캐시를 활용하여 읽기 대역폭을 높이고 연산 속도를 향상시키는 역할을 합니다.\n",
        "\n",
        "\n",
        "2. 이미지 크기 | 커널 크기 | Naive | Optimal | 성능 향상 비율\n",
        "-|-|-|-|-\n",
        "4096x4096 | 3x3 | 2.13ms | 2.29ms | 약 -7.2%\n",
        "4096x4096 | 7x7 | 6.0ms | 4.33ms | 약28.6%\n",
        "4096x4096 | 11x11 | 13.05ms | 7.52ms | 약42.3%\n",
        "1024x1024 | 5x5 | 0.34ms | 0.24ms | 약28.8%\n",
        "2048x2048 | 5x5 | 1.09ms | 0.84ms | 약22.7%\n",
        "8192x8192 | 5x5 | 15.03ms | 13.43ms | 약10.6%\n",
        "\n",
        "이미지 크기가 4096x4096으로 고정되었을 때, 커널이 커질수록 성능 향상 폭이 커졌습니다. 이는 커널이 커질수록 데이터 재사용 횟수가 기하급수적으로 증가하기 때문에, 전역 메모리 접근을 줄이고 공유 메모리를 활용하는 방식이 잘 사용되었습니다.\n",
        "\n",
        "작은 커널(k=3)에서는 최적화 버전이 Naive보다 다소 느리게 나타났습니다. k=3 수준에서는 데이터 재사용 이득보다 공유 메모리에 경계 영역을 계산하여 로드하고 thread 간 동기화를 수행하는 오버헤드가 더 크기 때문으로 생각됩니다.\n",
        "\n",
        "최종 결론: 본 실험을 통해 Shared Memory Tiling과 Constant Memory를 결합한 최적화 방식은 연산 집약적이고 데이터 재사용이 많은 큰 연산에서 매우 효과적임을 확인했습니다. 다만, 커널 크기가 매우 작은 경우에는 오버헤드를 고려하여 Naive 방식을 사용하는 것이 유리할 수 있습니다."
      ],
      "metadata": {
        "id": "T9yXTo4o_N4z"
      }
    }
  ]
}